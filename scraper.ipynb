{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.error import HTTPError, URLError\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_reviews(url, opener, limit, stars=[], reviews=[], titles=[]):\n",
    "    \n",
    "    regex = re.compile('^customer_review')\n",
    "    \n",
    "    try:\n",
    "        response = opener.open(url)\n",
    "        bs = BeautifulSoup(response, 'html.parser')\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "    except URLError as e:\n",
    "        print('The server could not be find.')\n",
    "    \n",
    "    try: # Get link to reviews\n",
    "\n",
    "        sublink = bs.find('a', {'data-hook': 'see-all-reviews-link-foot'}).attrs['href']\n",
    "        reviews_link = f'https://www.amazon.com{sublink}' \n",
    "        \n",
    "    except: # There is no reviews button (there is no review)\n",
    "        print('There is no reviews button')\n",
    "        \n",
    "    else:\n",
    "\n",
    "        # Go to reviews\n",
    "        try:\n",
    "            response = opener.open(reviews_link)\n",
    "            bs = BeautifulSoup(response, 'html.parser')\n",
    "        except HTTPError as e:\n",
    "            print(e)\n",
    "        except URLError as e:\n",
    "            print('The server could not be find.')\n",
    "\n",
    "        while True:\n",
    "\n",
    "            if len(stars) >= limit:\n",
    "                stars = stars[:limit]\n",
    "                reviews = reviews[:limit]\n",
    "                titles = titles[:limit]\n",
    "                break\n",
    "            \n",
    "            tags_array = bs.find_all('div', {'id': regex})\n",
    "\n",
    "            for tag in tags_array:\n",
    "\n",
    "                try: # Reviews not from USA have different tag structure\n",
    "                    star = tag.select(\"div[class='a-row'] > a[class='a-link-normal']\")[0].attrs['title'][0]\n",
    "                    review = tag.find('div', {'class': 'a-row a-spacing-small review-data'}).span.span.get_text()\n",
    "                    #title = tag.find('a', {'data-hook': 'review-title'}).span\n",
    " \n",
    "                    title = tag.find('a', {'data-hook': 'review-title'}).find('span', class_=None)\n",
    "\n",
    "                    if title == None:  # skip reviews not in English\n",
    "                        continue\n",
    "                    title = title.get_text()\n",
    "                    \n",
    "                    star = star.replace(',', '') # We will save data in csv\n",
    "                    review = review.replace(',', '')\n",
    "                    title = title.replace(',', '')\n",
    "                    \n",
    "                    stars.append(star)\n",
    "                    reviews.append(review)\n",
    "                    titles.append(title)\n",
    "                except:\n",
    "                    print('Not from USA')\n",
    "\n",
    "\n",
    "            try: # Go to the next reviews page\n",
    "\n",
    "                sublink_reviews = list(bs.find('li', {'class': 'a-last'}).children)[0].attrs['href']\n",
    "\n",
    "                next_reviews = f'https://www.amazon.com{sublink_reviews}'\n",
    "\n",
    "            except:\n",
    "                print('This is the last reviews page!')\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                try:\n",
    "                    response = opener.open(next_reviews)\n",
    "                    bs = BeautifulSoup(response, 'html.parser')\n",
    "                except HTTPError as e:\n",
    "                    print(e)\n",
    "                except URLError as e:\n",
    "                    print('The server could not be find.')\n",
    "            \n",
    "    return stars, reviews, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_product(product, opener, limit=50, links=[]):\n",
    "    \n",
    "    formatted_product = product.replace(' ', '+')\n",
    "    \n",
    "    try:\n",
    "        response = opener.open(f'https://www.amazon.com/s?k={formatted_product}')\n",
    "        bs = BeautifulSoup(response, 'html.parser')\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "    except URLError as e:\n",
    "        print('The server could not be find.')\n",
    "    \n",
    "    product_links = bs.find_all('a', {'class': 'a-link-normal s-no-outline'}, limit=limit)\n",
    "    \n",
    "    for link in product_links:\n",
    "        if link not in links:\n",
    "            links.append(f\"https://www.amazon.com/{link.attrs['href']}\")\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        if len(links) < limit:\n",
    "            try:\n",
    "                get_next = bs.find('a', {'class': 's-pagination-next'}).attrs['href']\n",
    "                         \n",
    "            except:\n",
    "                print(\"It's the last product page!\")\n",
    "                break\n",
    "                         \n",
    "            else:   \n",
    "                         \n",
    "                next_link = f\"https://www.amazon.com/{get_next}\"\n",
    "                \n",
    "                try: # Go to next products page\n",
    "                    response = opener.open(next_link)\n",
    "                    bs = BeautifulSoup(response, 'html.parser')\n",
    "                except HTTPError as e:\n",
    "                    print(e)\n",
    "                except URLError as e:\n",
    "                    print('The server could not be find.')\n",
    "                else: # Get product links\n",
    "                    product_links = bs.find_all('a', {'class': 'a-link-normal s-no-outline'}, limit=limit)\n",
    "\n",
    "                    for link in product_links:\n",
    "                        if link not in links:\n",
    "                            links.append(f\"https://www.amazon.com/{link.attrs['href']}\")\n",
    "        else:\n",
    "            break\n",
    "                     \n",
    "    return links[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrap(products:list, reviews_count:int=200, limit:int=500):\n",
    "    \n",
    "    links = []\n",
    "    stars = []\n",
    "    reviews = []\n",
    "    titles = []\n",
    "    \n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "    \n",
    "    for product in products:\n",
    "        \n",
    "        if len(links) == limit:\n",
    "            print(f'links founded: {links}')\n",
    "            break\n",
    "        \n",
    "        links = search_product(product, opener, limit=limit, links=links)\n",
    "\n",
    "    for link in links:\n",
    "        stars, reviews, titles = loop_reviews(link, opener=opener, limit=reviews_count, stars=stars, reviews=reviews, titles=titles)\n",
    "        \n",
    "    return stars, reviews, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(stars, reviews, titles, file:str='myfile.csv'):\n",
    "    \n",
    "    file = open(file, 'w+')\n",
    "    \n",
    "    try:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(('stars', 'reviews', 'titles'))\n",
    "        for i in range(len(stars)):\n",
    "            writer.writerow((stars[i], reviews[i], titles[i]))\n",
    "    finally:\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = time.time()\n",
    "\n",
    "# stars, reviews, titles = scrap(['cloths'], reviews_count=1000, limit=5)\n",
    "\n",
    "# save_csv(stars, reviews, titles)\n",
    "\n",
    "# b = time.time()\n",
    "\n",
    "# print(b-a)\n",
    "\n",
    "# print('just not to print return')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
